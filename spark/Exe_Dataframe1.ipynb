{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exe_Dataframe1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Domineice/Bigdata-class/blob/main/Exe_Dataframe1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U8OiIuROjxU",
        "outputId": "4f2ed4b8-fe02-4f36-b6ed-a1748141baee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=9dd62f0e527d26cfb57cec28a11e537f56483e805e3bc88e14b9bfde99bd97df\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dMBs65Te8cis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "zUpuyBRHPA1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "uvpcGWJ0PEmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()"
      ],
      "metadata": {
        "id": "S86JO9hFdlHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QoT97rvNbH_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('A',1),('B',2),('C',3)]"
      ],
      "metadata": {
        "id": "csniE1o4dxUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize(data)"
      ],
      "metadata": {
        "id": "0VDeUiCyek8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJiKII1s5Q9e",
        "outputId": "1180dccc-aa5f-4e66-c756-700a30736e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 1), ('B', 2), ('C', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataFrame"
      ],
      "metadata": {
        "id": "ANxDTDZFbVJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD=rdd.toDF()\n",
        "dfFromRDD.printSchema()\n",
        "dfFromRDD.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCDhf-KKeeEW",
        "outputId": "d96d89d9-f7a2-413f-bc0d-2bd785121456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---+---+\n",
            "| _1| _2|\n",
            "+---+---+\n",
            "|  A|  1|\n",
            "|  B|  2|\n",
            "|  C|  3|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header=['word','count']"
      ],
      "metadata": {
        "id": "DDQODxfyYkxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1=rdd.toDF(header)\n",
        "dfFromRDD1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tuGv7EpPXHm",
        "outputId": "f5fae7b4-de07-4d86-ada7-7f6156b4f25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- word: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw5ccDy4d1Bq",
        "outputId": "e03c766c-78c6-4851-d6cc-6cc397761a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "|   A|    1|\n",
            "|   B|    2|\n",
            "|   C|    3|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option2: Import RDD with column"
      ],
      "metadata": {
        "id": "oXlLli6NfnsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD2=spark.createDataFrame(rdd).toDF(*header)"
      ],
      "metadata": {
        "id": "EJpbYATWeAk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD2.printSchema()"
      ],
      "metadata": {
        "id": "Snul2oOGeNip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da3326a-9c54-4295-8267-ee9b18371a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- key: string (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD2.show()"
      ],
      "metadata": {
        "id": "qSkThO4TeNN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52813de1-9e01-4330-f087-0431a62a5303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|value|\n",
            "+---+-----+\n",
            "|  A|    1|\n",
            "|  B|    2|\n",
            "|  C|    3|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option3: Import the data directly"
      ],
      "metadata": {
        "id": "rJcd-4MmfrJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD3=spark.createDataFrame(data).toDF(*header)\n",
        "dfFromRDD3.printSchema()\n",
        "dfFromRDD3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-BIwJ08flc_",
        "outputId": "2d687df4-2b8d-4c26-ee04-29f9f0655081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- key: string (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            "\n",
            "+---+-----+\n",
            "|key|value|\n",
            "+---+-----+\n",
            "|  A|    1|\n",
            "|  B|    2|\n",
            "|  C|    3|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "3AdPMfD7e2nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import DataFrame"
      ],
      "metadata": {
        "id": "dSoEfg4yePNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "path = '/content/drive/My Drive/bigdata/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1sCbSmZRQjV",
        "outputId": "ddff26df-e982-492d-cf8d-5b16535bbe4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading csv files"
      ],
      "metadata": {
        "id": "HXiVPzGFYoeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames Import example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QTYLEGhkhwSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "7e32f63e-c2b5-4cd3-e2fa-e87660f131c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-08a1b6be03c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating a spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python Spark DataFrames Import example\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.some.config.option\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some-value\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = spark.read.csv(path+\"students.csv\")\n"
      ],
      "metadata": {
        "id": "zZb6frHBYoCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.show()"
      ],
      "metadata": {
        "id": "fcBn8utY8ukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = spark.read.csv(path+\"students.csv\",header='true')\n",
        "dataframe.show()"
      ],
      "metadata": {
        "id": "XBlFjV7KYnjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.printSchema()\n"
      ],
      "metadata": {
        "id": "sKqOO6KbZRLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = spark.read.csv(path+\"students.csv\",header='true',inferSchema=True)\n",
        "dataframe.show()\n",
        "dataframe.printSchema()\n"
      ],
      "metadata": {
        "id": "w97qEnAVZRON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Column Name\n",
        "dataframe.columns"
      ],
      "metadata": {
        "id": "5OP--Q4ph3mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(path+\"people.json\").cache()"
      ],
      "metadata": {
        "id": "alIo8KxpRtqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "7SFwM5iqR4Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "s6Y6JhVdYcat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**:** Explore the data using DataFrame** functions and SparkSQL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this section, we explore the datasets using functions both from dataframes as well as corresponding SQL queries using sparksql. Note the different ways to achieve the same task!# New Section"
      ],
      "metadata": {
        "id": "zlHD7jAZSCuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "df.select(\"age\").show()\n",
        "df.select(df[\"name\"]).show()\n"
      ],
      "metadata": {
        "id": "JAIvIVe3SHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createOrReplaceTempView(\"people\")\n"
      ],
      "metadata": {
        "id": "YE5oBcKJfpT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name,age FROM people\").show()"
      ],
      "metadata": {
        "id": "5KJ99QVP-7au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(\"age\").show()\n",
        "spark.sql(\"SELECT * FROM people order by age desc\").show()"
      ],
      "metadata": {
        "id": "9JI0Rp8vi4g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumnRenamed('name','Students_name').show()"
      ],
      "metadata": {
        "id": "-lCcJCvPAcQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumnRenamed('Students_name','name').show()"
      ],
      "metadata": {
        "id": "9NgunCEdAhSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new=df.withColumn('Graduation age',df['age']+1)\n",
        "df_new.show()\n"
      ],
      "metadata": {
        "id": "fxIJdmb4AvON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.drop('Graduation age').show()"
      ],
      "metadata": {
        "id": "-XVe82RrBImB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform basic filtering\n",
        "\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT * FROM people WHERE age > 21\").show()"
      ],
      "metadata": {
        "id": "F9Y6ADpYSJcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfom basic aggregation of data\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
      ],
      "metadata": {
        "id": "WdGt_ffySS6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n"
      ],
      "metadata": {
        "id": "9D4vkqosS-Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Exercise1** read the people2.json file into the notebook, load it into a dataframe and apply SQL operations to determine the average age in our people2 file."
      ],
      "metadata": {
        "id": "zV3_Efi5Tka5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 1\n",
        "# df = spark.read...\n",
        "df1 = spark.read.json(path+\"people2.json\")\n",
        "# df.createTempView..\n",
        "# spark.sql(\"SELECT ...\")\n",
        "from pyspark.sql import functions as f\n",
        "df1.createOrReplaceTempView(\"student\")\n"
      ],
      "metadata": {
        "id": "qs5JRyvsTDOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise2** read the people2.json file into the notebook, load it into a dataframe and apply SQL operations to determine NUMBER OF PEOPLE IN EACH PROGRAM/ AVERAGE  IN EACH PROGRAM\n"
      ],
      "metadata": {
        "id": "GSY--1XyrQGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby('major').agg(f.count('*'),f.avg('age')).show()"
      ],
      "metadata": {
        "id": "Na56XL9jqhEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4b6dec-dedb-4a63-adce-3d940a16682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------+\n",
            "| major|count(1)|avg(age)|\n",
            "+------+--------+--------+\n",
            "|EGCO/M|       5|   24.75|\n",
            "|  EGCO|       2|    22.0|\n",
            "+------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df1.na.fill(df1.select(f.mean(df['age'])).collect()[0][0],['age'])\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrEwZSLjLHd9",
        "outputId": "e53110b3-82d2-4e33-c3c0-22bbc20b94c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----------+\n",
            "| Major| age|       name|\n",
            "+------+----+-----------+\n",
            "|EGCO/M|  30|  Nartdanai|\n",
            "|EGCO/M|  29|      Tanut|\n",
            "|EGCO/M|null|  Nattapark|\n",
            "|EGCO/M|  20|  Weerawich|\n",
            "|  EGCO|  22|Threerapong|\n",
            "|EGCO/M|  20|  Weerawich|\n",
            "|  EGCO|  22|Threerapong|\n",
            "+------+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KTEXrWxLnVb",
        "outputId": "28028bd8-1a10-4d16-e4dd-ae4d20569c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+---+-----+---+\n",
            "|name|age|gpa|Major|age|\n",
            "+----+---+---+-----+---+\n",
            "+----+---+---+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def UpperCase(str):\n",
        "  return str.upper()"
      ],
      "metadata": {
        "id": "V0wzm21iQCZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "upperCaseUDF=udf(lambda z: UpperCase(z))"
      ],
      "metadata": {
        "id": "wkqT7o-sQIKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df1.withColumn(\"Uppercase Name\", upperCaseUDF(\"name\"))"
      ],
      "metadata": {
        "id": "LXbJgeW9QTrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQishH3-QajJ",
        "outputId": "150c418b-ac9b-493e-8c55-ed3b1edbc503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----------+--------------+\n",
            "| Major| age|       name|Uppercase Name|\n",
            "+------+----+-----------+--------------+\n",
            "|EGCO/M|  30|  Nartdanai|     NARTDANAI|\n",
            "|EGCO/M|  29|      Tanut|         TANUT|\n",
            "|EGCO/M|null|  Nattapark|     NATTAPARK|\n",
            "|EGCO/M|  20|  Weerawich|     WEERAWICH|\n",
            "|  EGCO|  22|Threerapong|   THREERAPONG|\n",
            "|EGCO/M|  20|  Weerawich|     WEERAWICH|\n",
            "|  EGCO|  22|Threerapong|   THREERAPONG|\n",
            "+------+----+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def std(input,mean,sd):\n",
        "  return (input-mean)/sd\n",
        "\n",
        "stdudf=udf(lambda )\n"
      ],
      "metadata": {
        "id": "sMUaFy7EQ6E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df1.withColumn(\"std\", stdudf('age',10,20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "47gYlg-9Rl60",
        "outputId": "544788b1-07ee-46c2-e5ab-25b31ecc492a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-620305e6abf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"std\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stdudf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise3** Remove Duplicate"
      ],
      "metadata": {
        "id": "qKLP8w9HzkIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 3"
      ],
      "metadata": {
        "id": "EwFT4ZVWziDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise4** REMOVE NULL VALUE or use average value to fill in the NULL"
      ],
      "metadata": {
        "id": "hpg1t_2-dXvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 4"
      ],
      "metadata": {
        "id": "fxdRBdY4bODa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#close session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ddVV46wkWTpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching Data**\n",
        "This simple example shows how to create an RDD and cache it. Notice the 10x speed improvement! If you wish to see the actual computation time."
      ],
      "metadata": {
        "id": "pnTIV9pqJzmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "test = sc.parallelize(range(1,50000),4)\n",
        "test.cache()\n",
        "\n",
        "t1 = time.time()\n",
        "# first count will trigger evaluation of count *and* cache\n",
        "count1 = test.count()\n",
        "dt1 = time.time() - t1\n",
        "print(\"dt1: \", dt1)\n",
        "\n",
        "\n",
        "t2 = time.time()\n",
        "# second count operates on cached data only\n",
        "count2 = test.count()\n",
        "dt2 = time.time() - t2\n",
        "print(\"dt2: \", dt2)\n",
        "\n",
        "#test.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-dOeKfrEI8p",
        "outputId": "9d95234a-41f2-4c7e-c744-576f33a48297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt1:  0.1692793369293213\n",
            "dt2:  0.08459901809692383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sWhghCezLe1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}